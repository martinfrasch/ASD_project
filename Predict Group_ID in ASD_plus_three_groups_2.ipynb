{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Predicting Group_ID in Control, ASD, Conduct Problems and Depression cohorts using HRV measures"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "### Notebook automatically generated our model"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Model XGBoost, trained on 2019-04-26 00:19:18."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "#### Generated on 2019-05-02 03:17:06.691733"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "#### Warning"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps\n",
        "of training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not\n",
        "replicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine\n",
        "learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Let\u0027s start with importing the required libs :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import dataiku\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import dataiku.core.pandasutils as pdu\n",
        "from dataiku.doctor.preprocessing import PCA\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "And tune pandas display options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "pd.set_option(\u0027display.width\u0027, 3000)\n",
        "pd.set_option(\u0027display.max_rows\u0027, 200)\n",
        "pd.set_option(\u0027display.max_columns\u0027, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Importing base data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The first step is to get our machine learning dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# We apply the preparation that you defined. You should not modify this.\n",
        "preparation_steps \u003d []\n",
        "preparation_output_schema \u003d {u\u0027userModified\u0027: False, u\u0027columns\u0027: [{u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027Group_ID\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_SDNN\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_RMSSD\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_SD1\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_SD2\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_Sample Entropy\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_Fuzzy Entropy\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_moment coeff of skewness\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_mode skewness\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_median skewness\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_LF power\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_HF power\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_RR\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_DET\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_ENTR\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027fet_L\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027PRSA_fetal\u0027}]}\n",
        "\n",
        "ml_dataset_handle \u003d dataiku.Dataset(\u0027ASD_plus_three_groups_UPDATED_ASD_code_vs_rest\u0027)\n",
        "ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n",
        "%time ml_dataset \u003d ml_dataset_handle.get_dataframe(limit \u003d 100000)\n",
        "\n",
        "print (\u0027Base data has %i rows and %i columns\u0027 % (ml_dataset.shape[0], ml_dataset.shape[1]))\n",
        "# Five first records\",\n",
        "ml_dataset.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Initial data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The preprocessing aims at making the dataset compatible with modeling.\n",
        "At the end of this step, we will have a matrix of float numbers, with no missing values.\n",
        "We\u0027ll use the features and the preprocessing steps defined in Models.\n",
        "\n",
        "Let\u0027s only keep selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "ml_dataset \u003d ml_dataset[[u\u0027fet_SDNN\u0027, u\u0027fet_Fuzzy Entropy\u0027, u\u0027fet_ENTR\u0027, u\u0027fet_SD2\u0027, u\u0027fet_median skewness\u0027, u\u0027PRSA_fetal\u0027, u\u0027fet_mode skewness\u0027, u\u0027fet_L\u0027, u\u0027Group_ID\u0027, u\u0027fet_RR\u0027, u\u0027fet_moment coeff of skewness\u0027, u\u0027fet_SD1\u0027, u\u0027fet_RMSSD\u0027, u\u0027fet_LF power\u0027, u\u0027fet_Sample Entropy\u0027, u\u0027fet_DET\u0027, u\u0027fet_HF power\u0027]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Let\u0027s first coerce categorical columns into unicode, numerical features into floats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# astype(\u0027unicode\u0027) does not work as expected\n",
        "\n",
        "def coerce_to_unicode(x):\n",
        "    if sys.version_info \u003c (3, 0):\n",
        "        if isinstance(x, str):\n",
        "            return unicode(x,\u0027utf-8\u0027)\n",
        "        else:\n",
        "            return unicode(x)\n",
        "    else:\n",
        "        return str(x)\n",
        "\n",
        "\n",
        "categorical_features \u003d []\n",
        "numerical_features \u003d [u\u0027fet_SDNN\u0027, u\u0027fet_Fuzzy Entropy\u0027, u\u0027fet_ENTR\u0027, u\u0027fet_SD2\u0027, u\u0027fet_median skewness\u0027, u\u0027PRSA_fetal\u0027, u\u0027fet_mode skewness\u0027, u\u0027fet_L\u0027, u\u0027fet_RR\u0027, u\u0027fet_moment coeff of skewness\u0027, u\u0027fet_SD1\u0027, u\u0027fet_RMSSD\u0027, u\u0027fet_LF power\u0027, u\u0027fet_Sample Entropy\u0027, u\u0027fet_DET\u0027, u\u0027fet_HF power\u0027]\n",
        "text_features \u003d []\n",
        "from dataiku.doctor.utils import datetime_to_epoch\n",
        "for feature in categorical_features:\n",
        "    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\n",
        "for feature in text_features:\n",
        "    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\n",
        "for feature in numerical_features:\n",
        "    if ml_dataset[feature].dtype \u003d\u003d np.dtype(\u0027M8[ns]\u0027):\n",
        "        ml_dataset[feature] \u003d datetime_to_epoch(ml_dataset[feature])\n",
        "    else:\n",
        "        ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027double\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We are now going to handle the target variable and store it in a new variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "target_map \u003d {u\u00271\u0027: 2, u\u00273\u0027: 0, u\u00272\u0027: 1}\n",
        "ml_dataset[\u0027__target__\u0027] \u003d ml_dataset[\u0027Group_ID\u0027].map(str).map(target_map)\n",
        "del ml_dataset[\u0027Group_ID\u0027]\n",
        "\n",
        "\n",
        "# Remove rows for which the target is unknown.\n",
        "ml_dataset \u003d ml_dataset[~ml_dataset[\u0027__target__\u0027].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Cross-validation strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The dataset needs to be split into 2 new sets, one that will be used for training the model (train set)\n",
        "and another that will be used to test its generalization capability (test set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "This is a simple cross-validation strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train, test \u003d pdu.split_train_valid(ml_dataset, prop\u003d0.8)\n",
        "print (\u0027Train data has %i rows and %i columns\u0027 % (train.shape[0], train.shape[1]))\n",
        "print (\u0027Test data has %i rows and %i columns\u0027 % (test.shape[0], test.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Features preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The first thing to do at the features level is to handle the missing values.\n",
        "Let\u0027s reuse the settings defined in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "drop_rows_when_missing \u003d []\n",
        "impute_when_missing \u003d [{\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_SDNN\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_Fuzzy Entropy\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_ENTR\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_SD2\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_median skewness\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027PRSA_fetal\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_mode skewness\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_L\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_RR\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_moment coeff of skewness\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_SD1\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_RMSSD\u0027}, {\u0027impute_with\u0027: u\u0027MEDIAN\u0027, \u0027feature\u0027: u\u0027fet_LF power\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_Sample Entropy\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_DET\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027fet_HF power\u0027}]\n",
        "\n",
        "# Features for which we drop rows with missing values\"\n",
        "for feature in drop_rows_when_missing:\n",
        "    train \u003d train[train[feature].notnull()]\n",
        "    test \u003d test[test[feature].notnull()]\n",
        "    print (\u0027Dropped missing records in %s\u0027 % feature)\n",
        "\n",
        "# Features for which we impute missing values\"\n",
        "for feature in impute_when_missing:\n",
        "    if feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEAN\u0027:\n",
        "        v \u003d train[feature[\u0027feature\u0027]].mean()\n",
        "    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEDIAN\u0027:\n",
        "        v \u003d train[feature[\u0027feature\u0027]].median()\n",
        "    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CREATE_CATEGORY\u0027:\n",
        "        v \u003d \u0027NULL_CATEGORY\u0027\n",
        "    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MODE\u0027:\n",
        "        v \u003d train[feature[\u0027feature\u0027]].value_counts().index[0]\n",
        "    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CONSTANT\u0027:\n",
        "        v \u003d feature[\u0027value\u0027]\n",
        "    train[feature[\u0027feature\u0027]] \u003d train[feature[\u0027feature\u0027]].fillna(v)\n",
        "    test[feature[\u0027feature\u0027]] \u003d test[feature[\u0027feature\u0027]].fillna(v)\n",
        "    print (\u0027Imputed missing values in feature %s with value %s\u0027 % (feature[\u0027feature\u0027], coerce_to_unicode(v)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We can now handle the categorical features (still using the settings defined in Models):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Let\u0027s rescale numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "rescale_features \u003d {u\u0027fet_RMSSD\u0027: u\u0027AVGSTD\u0027, u\u0027fet_SDNN\u0027: u\u0027AVGSTD\u0027, u\u0027fet_Fuzzy Entropy\u0027: u\u0027AVGSTD\u0027, u\u0027fet_ENTR\u0027: u\u0027AVGSTD\u0027, u\u0027fet_RR\u0027: u\u0027AVGSTD\u0027, u\u0027fet_median skewness\u0027: u\u0027AVGSTD\u0027, u\u0027PRSA_fetal\u0027: u\u0027AVGSTD\u0027, u\u0027fet_mode skewness\u0027: u\u0027AVGSTD\u0027, u\u0027fet_L\u0027: u\u0027AVGSTD\u0027, u\u0027fet_moment coeff of skewness\u0027: u\u0027AVGSTD\u0027, u\u0027fet_SD1\u0027: u\u0027AVGSTD\u0027, u\u0027fet_SD2\u0027: u\u0027AVGSTD\u0027, u\u0027fet_LF power\u0027: u\u0027AVGSTD\u0027, u\u0027fet_Sample Entropy\u0027: u\u0027AVGSTD\u0027, u\u0027fet_DET\u0027: u\u0027AVGSTD\u0027, u\u0027fet_HF power\u0027: u\u0027AVGSTD\u0027}\n",
        "for (feature_name, rescale_method) in rescale_features.items():\n",
        "    if rescale_method \u003d\u003d \u0027MINMAX\u0027:\n",
        "        _min \u003d train[feature_name].min()\n",
        "        _max \u003d train[feature_name].max()\n",
        "        scale \u003d _max - _min\n",
        "        shift \u003d _min\n",
        "    else:\n",
        "        shift \u003d train[feature_name].mean()\n",
        "        scale \u003d train[feature_name].std()\n",
        "    if scale \u003d\u003d 0.:\n",
        "        del train[feature_name]\n",
        "        del test[feature_name]\n",
        "        print (\u0027Feature %s was dropped because it has no variance\u0027 % feature_name)\n",
        "    else:\n",
        "        print (\u0027Rescaled %s\u0027 % feature_name)\n",
        "        train[feature_name] \u003d (train[feature_name] - shift).astype(np.float64) / scale\n",
        "        test[feature_name] \u003d (test[feature_name] - shift).astype(np.float64) / scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Before actually creating our model, we need to split the datasets into their features and labels parts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train_X \u003d train.drop(\u0027__target__\u0027, axis\u003d1)\n",
        "test_X \u003d test.drop(\u0027__target__\u0027, axis\u003d1)\n",
        "\n",
        "train_Y \u003d np.array(train[\u0027__target__\u0027])\n",
        "test_Y \u003d np.array(test[\u0027__target__\u0027])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now we can finally create our model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "clf \u003d xgb.XGBClassifier(\n",
        "                    max_depth\u003d3,\n",
        "                    learning_rate\u003d0.2,\n",
        "                    gamma\u003d0.0,\n",
        "                    min_child_weight\u003d0.0,\n",
        "                    max_delta_step\u003d0.0,\n",
        "                    subsample\u003d1.0,\n",
        "                    colsample_bytree\u003d1.0,\n",
        "                    colsample_bylevel\u003d1.0,\n",
        "                    reg_alpha\u003d0.0,\n",
        "                    reg_lambda\u003d1.0,\n",
        "                    n_estimators\u003d1,\n",
        "                    silent\u003d0,\n",
        "                    nthread\u003d4,\n",
        "                    scale_pos_weight\u003d1.0,\n",
        "                    base_score\u003d0.5,\n",
        "                    seed\u003d1337,\n",
        "                    missing\u003dNone,\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "... And train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%time clf.fit(train_X, train_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Build up our result dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The model is now being trained, we can apply it to our test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%time _predictions \u003d clf.predict(test_X)\n",
        "%time _probas \u003d clf.predict_proba(test_X)\n",
        "predictions \u003d pd.Series(data\u003d_predictions, index\u003dtest_X.index, name\u003d\u0027predicted_value\u0027)\n",
        "cols \u003d [\n",
        "    u\u0027probability_of_value_%s\u0027 % label\n",
        "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
        "]\n",
        "probabilities \u003d pd.DataFrame(data\u003d_probas, index\u003dtest_X.index, columns\u003dcols)\n",
        "\n",
        "# Build scored dataset\n",
        "results_test \u003d test_X.join(predictions, how\u003d\u0027left\u0027)\n",
        "results_test \u003d results_test.join(probabilities, how\u003d\u0027left\u0027)\n",
        "results_test \u003d results_test.join(test[\u0027__target__\u0027], how\u003d\u0027left\u0027)\n",
        "results_test \u003d results_test.rename(columns\u003d {\u0027__target__\u0027: \u0027Group_ID\u0027})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "You can measure the model\u0027s accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
        "test_Y_ser \u003d pd.Series(test_Y)\n",
        "print (\u0027AUC value:\u0027, mroc_auc_score(test_Y_ser, _probas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We can also view the predictions directly.\n",
        "Since scikit-learn only predicts numericals, the labels have been mapped to 0,1,2 ...\n",
        "We need to \u0027reverse\u0027 the mapping to display the initial labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "inv_map \u003d { target_map[label] : label for label in target_map}\n",
        "predictions.map(inv_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "That\u0027s it. It\u0027s now up to you to tune your preprocessing, your algo, and your analysis !\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "name": "Predicting Group_ID in ASD_plus_three_groups_UPDATED_ASD_code_vs_rest"
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
